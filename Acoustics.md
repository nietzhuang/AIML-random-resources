# Random Acoustics resources from AI/ML Community.
* Working with Audio Data for Machine Learning in Python [[link](https://heartbeat.comet.ml/working-with-audio-signals-in-python-6c2bd63b2daf)]
* Text to Speech [[link](https://github.com/mozilla/TTS)]
* BERT Explained: A Complete Guide with Theory and Tutorial [[link](https://towardsml.wordpress.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/)]
* NLP.js [[link](https://github.com/axa-group/nlp.js)]
* nlp-with-transformers [[link](https://github.com/nlp-with-transformers/notebooks)]
* MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training [[link](https://aclanthology.org/2021.findings-acl.70/)]
* mozilla/TTS [[link](https://github.com/mozilla/TTS)]
* Audio Deep Learning Made Simple: Automatic Speech Recognition (ASR), How it Works [[link](https://towardsdatascience.com/audio-deep-learning-made-simple-automatic-speech-recognition-asr-how-it-works-716cfce4c706)]
* voice_datasets [[link](https://github.com/jim-schwoebel/voice_datasets)]
* Awesome-Speech-Pretraining [[link](https://github.com/ddlBoJack/Awesome-Speech-Pretraining)]
* Training Compute-Optimal Large Language Models: DeepMindâ€™s 70B Parameter Chinchilla Outperforms 530B Parameter Megatron-Turing [[link](https://medium.com/syncedreview/training-compute-optimal-large-language-models-deepminds-70b-parameter-chinchilla-outperforms-b6098d040265)]
* An introduction to transformers and Hugging Face [[link](https://towardsdatascience.com/an-introduction-to-transformers-and-hugging-face-13052ec9d72d)]
